{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unibo@SemEval Task 5: MAMI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiOIUFqNy0lS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "XymZBky5kEez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHkJyy3_k76h",
        "outputId": "05451b93-21a2-422b-c064-ee45171eece2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting bert-tensorflow\n",
            "  Downloading bert_tensorflow-1.0.4-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.43-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.49)\n",
            "Collecting botocore<1.25.0,>=1.24.43\n",
            "  Downloading botocore-1.24.43-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.43->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 36.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.43->boto3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.43 botocore-1.24.43 jmespath-1.0.0 s3transfer-0.5.2 sentencepiece-0.1.96 urllib3-1.25.11\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 49 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-5sp422_l\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-5sp422_l\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369328 sha256=0f2007438bcb2174ffd825409a169e42bbb4c419a8a5939878ff9f49715b80bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8m8u3r1g/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Collecting madgrad\n",
            "  Downloading madgrad-1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: madgrad\n",
            "Successfully installed madgrad-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install bert-tensorflow\n",
        "#!pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
        "#!pip install --upgrade --pre mmf\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install madgrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyuxCvvllARX",
        "outputId": "ce4ecf36-b7ce-4e1d-f555-253245f44298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vPUBOMlkaAu",
        "outputId": "dc863225-3e03-4f34-eec4-8de7335c1df4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from madgrad import MADGRAD\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "import clip\n",
        "import pickle\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import copy\n",
        "\n",
        "import transformers\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    MMBTConfig,\n",
        "    MMBTModel,\n",
        "    MMBTForClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "#Create a variable with available device, which will do all needed computations. We will need a GPU, so our device is CUDA.\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yXvH-xZu-HV"
      },
      "outputs": [],
      "source": [
        "#Here is the path for the images\n",
        "train_img = !unzip gdrive/MyDrive/training.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OMunQtxu0jy-",
        "outputId": "61ebee0a-a03f-42ee-87dc-948b92aeb33b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-79f351c0-3ce1-48fa-b273-bd0f127bfd96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>misogynous</th>\n",
              "      <th>shaming</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>objectification</th>\n",
              "      <th>violence</th>\n",
              "      <th>Text Transcription;;;;;;;;;;;;;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip;;;;;;;;;;;;;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O;;;;;;;;;;;;;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>WAITING FOR THE END OF THE COVID  imgflip.com;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>15003.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN ARE AROUND  imgflip.com;;;;;;;;;;;;;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>COOKING FOR MY WIFE  imgflip.com;;;;;;;;;;;;;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79f351c0-3ce1-48fa-b273-bd0f127bfd96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79f351c0-3ce1-48fa-b273-bd0f127bfd96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79f351c0-3ce1-48fa-b273-bd0f127bfd96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      file_name  ...                    Text Transcription;;;;;;;;;;;;;\n",
              "0         1.jpg  ...                         Milk Milk.zip;;;;;;;;;;;;;\n",
              "1        10.jpg  ...  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...\n",
              "2      1000.jpg  ...  BREAKING NEWS: Russia releases photo of DONALD...\n",
              "3     10000.jpg  ...          MAN SEEKING WOMAN Ignad 18 O;;;;;;;;;;;;;\n",
              "4     10006.jpg  ...  Me explaining the deep lore of. J.R.R. Tolkein...\n",
              "...         ...  ...                                                ...\n",
              "9995  15002.jpg  ...  WAITING FOR THE END OF THE COVID  imgflip.com;...\n",
              "9996  15003.jpg  ...   SMART WOMEN ARE AROUND  imgflip.com;;;;;;;;;;;;;\n",
              "9997  15004.jpg  ...  GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com;...\n",
              "9998  15005.jpg  ...      COOKING FOR MY WIFE  imgflip.com;;;;;;;;;;;;;\n",
              "9999  15006.jpg  ...  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...\n",
              "\n",
              "[10000 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#upload the dataset with file name, labels and text transcription\n",
        "data_train = pd.read_excel('memes.xlsx')\n",
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV6NayoV1EcM"
      },
      "outputs": [],
      "source": [
        "#let's keep only the 'misogynous' column, as it is the first class that we are going to train\n",
        "data_train = data_train.drop(['stereotype'], axis=1)\n",
        "data_train = data_train.drop(['shaming'], axis=1)\n",
        "data_train = data_train.drop(['objectification'], axis=1)\n",
        "data_train = data_train.drop(['violence'], axis=1)\n",
        "data_train = data_train.rename(columns={'Text Transcription;;;;;;;;;;;;;': 'text'})\n",
        "data_train = data_train.rename(columns={'file_name': 'img'})\n",
        "data_train = data_train.rename(columns={'misogynous': 'label'})\n",
        "#let's create the ID column\n",
        "data_train['id'] = data_train.index\n",
        "\n",
        "#let's do the same for the validation dataset\n",
        "#data_val = data_val.drop(['data_type'], axis=1)\n",
        "#data_val = data_val.rename(columns={'Text Transcription;;;;;;;;;;;;;': 'text'})\n",
        "#data_val = data_val.rename(columns={'file_name': 'img'})\n",
        "#data_val = data_val.rename(columns={'misogynous': 'label'})\n",
        "#id column\n",
        "#data_val['id'] = data_val.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHZNagDA119H"
      },
      "outputs": [],
      "source": [
        "#split train-val\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset in traning and validation(test)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "    data_train.index.values,\n",
        "    data_train.label.values,\n",
        "    test_size=0.10,\n",
        "    random_state=17,\n",
        "    stratify=data_train.label.values\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pfIeSWO92D1f",
        "outputId": "49786b04-da04-4317-d725-958f91517543"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c7e0c444-234f-48ff-ab75-46c4886d2155\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th>data_type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>train</th>\n",
              "      <td>4500</td>\n",
              "      <td>4500</td>\n",
              "      <td>4500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>train</th>\n",
              "      <td>4500</td>\n",
              "      <td>4500</td>\n",
              "      <td>4500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7e0c444-234f-48ff-ab75-46c4886d2155')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c7e0c444-234f-48ff-ab75-46c4886d2155 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c7e0c444-234f-48ff-ab75-46c4886d2155');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                  img  text    id\n",
              "label data_type                  \n",
              "0     train      4500  4500  4500\n",
              "      val         500   500   500\n",
              "1     train      4500  4500  4500\n",
              "      val         500   500   500"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Check datasets composition\n",
        "data_train['data_type'] = ['not_set'] * data_train.shape[0]\n",
        "data_train.loc[X_train, 'data_type'] = 'train'\n",
        "data_train.loc[X_val, 'data_type'] = 'val'\n",
        "data_train.groupby(['label', 'data_type']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz6eS_ik2gsZ"
      },
      "outputs": [],
      "source": [
        "data_train.to_excel('split_train_val.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "n_O9Cay_CBj8",
        "outputId": "d61d38ea-d547-47a1-97d5-af323168eebb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e81f42bc-4dc0-4f87-9d3a-77e9fda5f40c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>data_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip;;;;;;;;;;;;;</td>\n",
              "      <td>0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "      <td>2</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O;;;;;;;;;;;;;</td>\n",
              "      <td>3</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "      <td>4</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>WAITING FOR THE END OF THE COVID  imgflip.com;...</td>\n",
              "      <td>9995</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>15003.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN ARE AROUND  imgflip.com;;;;;;;;;;;;;</td>\n",
              "      <td>9996</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com;...</td>\n",
              "      <td>9997</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>COOKING FOR MY WIFE  imgflip.com;;;;;;;;;;;;;</td>\n",
              "      <td>9998</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
              "      <td>9999</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e81f42bc-4dc0-4f87-9d3a-77e9fda5f40c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e81f42bc-4dc0-4f87-9d3a-77e9fda5f40c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e81f42bc-4dc0-4f87-9d3a-77e9fda5f40c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0        img  ...    id data_type\n",
              "0              0      1.jpg  ...     0     train\n",
              "1              1     10.jpg  ...     1     train\n",
              "2              2   1000.jpg  ...     2       val\n",
              "3              3  10000.jpg  ...     3     train\n",
              "4              4  10006.jpg  ...     4     train\n",
              "...          ...        ...  ...   ...       ...\n",
              "9995        9995  15002.jpg  ...  9995     train\n",
              "9996        9996  15003.jpg  ...  9996     train\n",
              "9997        9997  15004.jpg  ...  9997     train\n",
              "9998        9998  15005.jpg  ...  9998     train\n",
              "9999        9999  15006.jpg  ...  9999     train\n",
              "\n",
              "[10000 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data = pd.read_excel('split_train_val.xlsx')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdEwjRRVCGcA"
      },
      "outputs": [],
      "source": [
        "data = data.dropna() #2 rows dropped\n",
        "data = data.drop('Unnamed: 0', axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC10o790CYBI"
      },
      "outputs": [],
      "source": [
        "train = data.loc[data['data_type'] == 'train']\n",
        "val = data.loc[data['data_type'] == 'val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH_H74GG2Om9"
      },
      "outputs": [],
      "source": [
        "# save json file for train\n",
        "train.to_json('train_mis.json', orient='records', lines=True)\n",
        "# same for validation\n",
        "val.to_json('val_mis.json', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6TB_1TDKzjAD",
        "outputId": "a71414b1-2b5a-40ee-f6c9-27ac16af33f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5caafb1a-6ee2-4a67-897d-ba5c67021270\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Milk Milk.zip;;;;;;;;;;;;;</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>MAN SEEKING WOMAN Ignad 18 O;;;;;;;;;;;;;</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10007.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>PICTOPHLE APP *Straight white malle starts tal...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8994</th>\n",
              "      <td>15002.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>WAITING FOR THE END OF THE COVID  imgflip.com;...</td>\n",
              "      <td>9995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>15003.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SMART WOMEN ARE AROUND  imgflip.com;;;;;;;;;;;;;</td>\n",
              "      <td>9996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>15004.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com;...</td>\n",
              "      <td>9997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>15005.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>COOKING FOR MY WIFE  imgflip.com;;;;;;;;;;;;;</td>\n",
              "      <td>9998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>15006.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...</td>\n",
              "      <td>9999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8999 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5caafb1a-6ee2-4a67-897d-ba5c67021270')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5caafb1a-6ee2-4a67-897d-ba5c67021270 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5caafb1a-6ee2-4a67-897d-ba5c67021270');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            img  label                                               text    id\n",
              "0         1.jpg      0                         Milk Milk.zip;;;;;;;;;;;;;     0\n",
              "1        10.jpg      1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...     1\n",
              "2     10000.jpg      0          MAN SEEKING WOMAN Ignad 18 O;;;;;;;;;;;;;     3\n",
              "3     10006.jpg      0  Me explaining the deep lore of. J.R.R. Tolkein...     4\n",
              "4     10007.jpg      0  PICTOPHLE APP *Straight white malle starts tal...     5\n",
              "...         ...    ...                                                ...   ...\n",
              "8994  15002.jpg      0  WAITING FOR THE END OF THE COVID  imgflip.com;...  9995\n",
              "8995  15003.jpg      0   SMART WOMEN ARE AROUND  imgflip.com;;;;;;;;;;;;;  9996\n",
              "8996  15004.jpg      0  GOOD GIRLS ARE BEHIND THE CORNER  imgflip.com;...  9997\n",
              "8997  15005.jpg      0      COOKING FOR MY WIFE  imgflip.com;;;;;;;;;;;;;  9998\n",
              "8998  15006.jpg      0  LISTEN TOMORROW WILL BE MONDAY imgflip.com FRO...  9999\n",
              "\n",
              "[8999 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#now we drag 'train.json' and 'val.json' into the folder 'dataset'. Let's have a look.\n",
        "data_train = pd.read_json('/content/dataset/train_mis.json', lines=True)\n",
        "data_train = data_train.drop('data_type', axis=1)\n",
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "O3XdFTst2tio",
        "outputId": "884c70fe-5b58-410d-9470-9f6755e17ae3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b7435f66-7ba7-4e04-834d-654589985330\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10016.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>IATE A WHOLE PIZZA HUT Posted By: Jorge Kerby ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10022.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>HM.. NEXT GAME! ARE YOU WINNING LOVE? 6000 ......</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10043.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>When Feminists realize that most of them \"MENs...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10048.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>Women: I hate my hair. No products work for me...</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>994.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>Is Maths related to Science?;;;;;;;;;;;;;</td>\n",
              "      <td>9938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>9949.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SFR 11:00 When I watch anime with my french we...</td>\n",
              "      <td>9947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>9961.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>SHE SAID HI TO ME serenayaraf PRO 2h @der_urs ...</td>\n",
              "      <td>9959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>9962.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>When the pussy so good you lose all track of t...</td>\n",
              "      <td>9960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>9982.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>MICHELLE OBAMA GLAMOUR SHOT memegenerator.net;...</td>\n",
              "      <td>9977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7435f66-7ba7-4e04-834d-654589985330')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7435f66-7ba7-4e04-834d-654589985330 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7435f66-7ba7-4e04-834d-654589985330');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           img  label                                               text    id\n",
              "0     1000.jpg      0  BREAKING NEWS: Russia releases photo of DONALD...     2\n",
              "1    10016.jpg      1  IATE A WHOLE PIZZA HUT Posted By: Jorge Kerby ...    14\n",
              "2    10022.jpg      0  HM.. NEXT GAME! ARE YOU WINNING LOVE? 6000 ......    19\n",
              "3    10043.jpg      1  When Feminists realize that most of them \"MENs...    37\n",
              "4    10048.jpg      0  Women: I hate my hair. No products work for me...    41\n",
              "..         ...    ...                                                ...   ...\n",
              "994    994.jpg      1          Is Maths related to Science?;;;;;;;;;;;;;  9938\n",
              "995   9949.jpg      0  SFR 11:00 When I watch anime with my french we...  9947\n",
              "996   9961.jpg      0  SHE SAID HI TO ME serenayaraf PRO 2h @der_urs ...  9959\n",
              "997   9962.jpg      1  When the pussy so good you lose all track of t...  9960\n",
              "998   9982.jpg      1  MICHELLE OBAMA GLAMOUR SHOT memegenerator.net;...  9977\n",
              "\n",
              "[999 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#let's see the val dataset as well\n",
        "data_val = pd.read_json('/content/dataset/val_mis.json', lines=True)\n",
        "data_val = data_val.drop('data_type', axis=1)\n",
        "data_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXHUOeoAd1c1"
      },
      "outputs": [],
      "source": [
        "#let's move also the folder TRAINING into dataset and then execute this\n",
        "!mv dataset/TRAINING/*jpg dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ONo6VnukoJy",
        "outputId": "3b049679-b3b7-4865-cdd6-46626df5b309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 402M/402M [00:06<00:00, 62.8MiB/s]\n"
          ]
        }
      ],
      "source": [
        "a#Load CLIP model and needed preprocessing.\n",
        "\n",
        "clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\n",
        "\n",
        "#Freeze weights of CLIP feature encoder, as we will not finetune it.\n",
        "\n",
        "for p in clip_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "#Initialize needed variables. \n",
        "\n",
        "num_image_embeds = 4\n",
        "num_labels = 1\n",
        "gradient_accumulation_steps = 20\n",
        "data_dir = '/content/dataset'\n",
        "max_seq_length = 80 \n",
        "max_grad_norm = 0.5\n",
        "train_batch_size = 16\n",
        "eval_batch_size = 16\n",
        "image_encoder_size = 288\n",
        "image_features_size = 640\n",
        "num_train_epochs = 5\n",
        "\n",
        "\n",
        "#Create a function that will prepare an image for CLIP encoder in a special manner. \n",
        "#This function will split image into three tiles (by height or width, depending on the aspect ratio of the image). \n",
        "#Finally we will get four vectors after encoding (one vector for each tile and one vector for whole image that was padded to square).\n",
        "\n",
        "def slice_image(im, desired_size):\n",
        "    '''\n",
        "    Resize and slice image\n",
        "    '''\n",
        "    old_size = im.size  \n",
        "\n",
        "    ratio = float(desired_size)/min(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "    im = im.resize(new_size, Image.ANTIALIAS)\n",
        "    \n",
        "    ar = np.array(im)\n",
        "    images = []\n",
        "    if ar.shape[0] < ar.shape[1]:\n",
        "        middle = ar.shape[1] // 2\n",
        "        half = desired_size // 2\n",
        "        \n",
        "        images.append(Image.fromarray(ar[:, :desired_size]))\n",
        "        images.append(Image.fromarray(ar[:, middle-half:middle+half]))\n",
        "        images.append(Image.fromarray(ar[:, ar.shape[1]-desired_size:ar.shape[1]]))\n",
        "    else:\n",
        "        middle = ar.shape[0] // 2\n",
        "        half = desired_size // 2\n",
        "        \n",
        "        images.append(Image.fromarray(ar[:desired_size, :]))\n",
        "        images.append(Image.fromarray(ar[middle-half:middle+half, :]))\n",
        "        images.append(Image.fromarray(ar[ar.shape[0]-desired_size:ar.shape[0], :]))\n",
        "\n",
        "    return images\n",
        "def resize_pad_image(im, desired_size):\n",
        "    '''\n",
        "    Resize and pad image to a desired size\n",
        "    '''\n",
        "    old_size = im.size  \n",
        "\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "    im = im.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "    # create a new image and paste the resized on it\n",
        "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
        "    new_im.paste(im, ((desired_size-new_size[0])//2,\n",
        "                        (desired_size-new_size[1])//2))\n",
        "\n",
        "    return new_im\n",
        "\n",
        "#Define a function, that will get image features from CLIP.\n",
        "\n",
        "class ClipEncoderMulti(nn.Module):\n",
        "    def __init__(self, num_embeds, num_features=image_features_size):\n",
        "        super().__init__()        \n",
        "        self.model = clip_model\n",
        "        self.num_embeds = num_embeds\n",
        "        self.num_features = num_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 4x3x288x288 -> 1x4x640\n",
        "        out = self.model.encode_image(x.view(-1,3,288,288))\n",
        "        out = out.view(-1, self.num_embeds, self.num_features).float()\n",
        "        return out  # Bx4x640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoUwgLFPlaNQ"
      },
      "outputs": [],
      "source": [
        "class JsonlDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n",
        "        self.data = [json.loads(l) for l in open(data_path)]\n",
        "        self.data_dir = os.path.dirname(data_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n",
        "        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
        "        sentence = sentence[:self.max_seq_length]\n",
        "\n",
        "        label = torch.FloatTensor([self.data[index][\"label\"]])\n",
        "\n",
        "        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
        "        sliced_images = slice_image(image, 288)\n",
        "        sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n",
        "        image = resize_pad_image(image, image_encoder_size)\n",
        "        image = np.array(self.transforms(image))\n",
        "        \n",
        "        sliced_images = [image] + sliced_images         \n",
        "        sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n",
        "\n",
        "        return {\n",
        "            \"image_start_token\": start_token,            \n",
        "            \"image_end_token\": end_token,\n",
        "            \"sentence\": sentence,\n",
        "            \"image\": sliced_images,\n",
        "            \"label\": label            \n",
        "        }\n",
        "\n",
        "    def get_label_frequencies(self):\n",
        "        label_freqs = Counter()\n",
        "        for row in self.data:\n",
        "            label_freqs.update([row[\"label\"]])\n",
        "        return label_freqs\n",
        "    \n",
        "    def get_labels(self):\n",
        "        labels = []\n",
        "        for row in self.data:\n",
        "            labels.append(row[\"label\"])\n",
        "        return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfh74-PHlhpI"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    lens = [len(row[\"sentence\"]) for row in batch]\n",
        "    bsz, max_seq_len = len(batch), max(lens)\n",
        "\n",
        "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "\n",
        "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
        "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
        "        mask_tensor[i_batch, :length] = 1\n",
        "    \n",
        "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
        "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
        "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
        "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
        "\n",
        "    return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, tgt_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojv9p_anlkLE"
      },
      "outputs": [],
      "source": [
        "def load_examples(tokenizer, evaluate=False):\n",
        "    path = os.path.join(data_dir, \"val_mis.json\" if evaluate else f\"train_mis.json\")\n",
        "    transforms = preprocess\n",
        "    dataset = JsonlDataset(path, tokenizer, transforms, max_seq_length - num_image_embeds - 2)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tg08b-8lm0W"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "    \n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "f22abfd353e84145b32c9641964512cd",
            "0ae8bb401c3c4ce39056b3c78421dc2c",
            "326e3806bb124bcf82f3c484b3b3a6bc",
            "33a01462767644c98991fbd52244c850",
            "70941d19c100463095a042c81489838f",
            "56e48f5cecb947b8a389860756392f0d",
            "ea1ad50969224ac0b95cbeacd3dd844a",
            "1b3ccf3b14ad4cf8a78c11581e451e42",
            "a88b16b5c7534eb199d3465c456f82db",
            "b96f11493fa042ba98a9bfe5e826d0c6",
            "ccae4ee3fcae49fcac9bfcd055fb7463",
            "8acb864f08684af0bfb499ced7d71696",
            "b56f4ea4d68d4c45b9c8f96f8fa0eded",
            "22f97440969d4ad3be7d5d5778ef59ef",
            "3d64f1e757d44293a1c85caaeeb22156",
            "142eb4fcc2bd422daf53f660a22b5e74",
            "e0f55bcf34eb4f2ca45082807ad22089",
            "a80476c0b56a4ed3b40a0caa2ccb116e",
            "34181a4045f744a69981ff7e50ef13d1",
            "ed6ebf7154664fea87322aceec7b2620",
            "b5d58c7319474457b56fa6e54267db8a",
            "ea7aed3e2d8a4f1c83cedd8ee008a6de"
          ]
        },
        "id": "Q1yNTfu3lpSN",
        "outputId": "29507e12-a5af-4fb9-ba9d-dc15c7501643"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f22abfd353e84145b32c9641964512cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8acb864f08684af0bfb499ced7d71696",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "transformer_config = AutoConfig.from_pretrained(model_name) \n",
        "transformer = AutoModel.from_pretrained(model_name, config=transformer_config)\n",
        "img_encoder = ClipEncoderMulti(num_image_embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "8fba2fc8c99d4064890e20b915f27d8f",
            "847937a40aeb45deb3bf0565da867979",
            "fde9573549b347a78076b0e84fc766ec",
            "2d0aefcaa23a429087057a9bb4908d20",
            "2c6ff551a67b4ae6a8f5fea3f6d3e2f4",
            "9742289add8540a39f342bf5da2a0284",
            "99a5123e5f6b4b4abe7bf658b6dd0592",
            "3ce78022fa9744a1bf17f6714aa66102",
            "55b33bc5bf6c4a44b1169334fd5f3b7e",
            "ee94b2b218e0438b91fe97876b386048",
            "ecfb530c5b0040059664fefe4392e00d",
            "9e3fa1f72e5a4ec98ad11aec20527773",
            "90982823ab514c39a40490d690c8ff5f",
            "600e1d7ecf104b7c90a328de70689afa",
            "0c5247d106dc468bb008ba79565ff4b5",
            "8e15193b634241149330962333366673",
            "060d33647f07413a8ae78d7ab6068628",
            "85deb064bad84810902bb52d0159ae38",
            "910d37f4c56a458dbaaa541581fee7b3",
            "e7bf50f1170a454894d1e3e44b63d01c",
            "5b26446e356448f2a0c8416ec03b1e51",
            "bd7265b660c54ab392329d7b10638379",
            "fe39fe6ae81a4cae9937e34af25254aa",
            "19eb13e8f7e242e2b2d12d98d183d625",
            "36e0723c3fbc45819edc3a4464bb005f",
            "7389292c13d446a5ba62fc29ee6203e8",
            "8d495a2ef473474399422d8dc9aa38fb",
            "3782d9d2364247d38e2e59e56f1a9aad",
            "945c8c3eaaff4d98bdfc02f4037e9cf5",
            "248cbb7cdcca46eea4af964de387fdcd",
            "1e1e7cf5ffd04c209d2a4d80b3c52fee",
            "81e8faa0bddd404cb3a3b87e17021f2d",
            "29e248f42c864175a7824870cfabf45a"
          ]
        },
        "id": "kwM7OUjelsgk",
        "outputId": "01a22f03-9855-4c03-e708-aa492e9f5913"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fba2fc8c99d4064890e20b915f27d8f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e3fa1f72e5a4ec98ad11aec20527773",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe39fe6ae81a4cae9937e34af25254aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auPS6oLIlxNL"
      },
      "outputs": [],
      "source": [
        "config = MMBTConfig(transformer_config, num_labels=num_labels, modal_hidden_size=image_features_size)\n",
        "model = MMBTForClassification(config, transformer, img_encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lla0JlnYl0P-"
      },
      "outputs": [],
      "source": [
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz3wMuAfl2yu"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_examples(tokenizer, evaluate=False)\n",
        "eval_dataset = load_examples(tokenizer, evaluate=True)   \n",
        "\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "eval_sampler = SequentialSampler(eval_dataset)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=train_sampler,\n",
        "        batch_size=train_batch_size,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "        eval_dataset, \n",
        "        sampler=eval_sampler, \n",
        "        batch_size=eval_batch_size, \n",
        "        collate_fn=collate_fn\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TdMUy3cY0-i"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h-aHbJDmhhf"
      },
      "outputs": [],
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = [\"bias\", \n",
        "            \"LayerNorm.weight\"\n",
        "           ]\n",
        "weight_decay = 0.0005\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "t_total = (len(train_dataloader) // gradient_accumulation_steps) * num_train_epochs\n",
        "warmup_steps = t_total // 10\n",
        "\n",
        "optimizer = MADGRAD(optimizer_grouped_parameters, lr=2e-4)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, warmup_steps, t_total\n",
        "    )\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TipYLQGDmyRp"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer, criterion, dataloader, tres = 0.5): \n",
        "    \n",
        "    # Eval!\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    proba = None\n",
        "    out_label_ids = None\n",
        "    for batch in dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            labels = batch[5]\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"input_modal\": batch[2],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"modal_start_tokens\": batch[3],\n",
        "                \"modal_end_tokens\": batch[4],\n",
        "                \"return_dict\": False\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "            tmp_eval_loss = criterion(logits, labels)\n",
        "            \n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            preds = torch.sigmoid(logits).detach().cpu().numpy() > tres\n",
        "            proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:            \n",
        "            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > tres, axis=0)\n",
        "            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "\n",
        "    result = {\n",
        "        \"loss\": eval_loss,\n",
        "        \"accuracy\": accuracy_score(out_label_ids, preds),\n",
        "        \"AUC\": roc_auc_score(out_label_ids, proba),\n",
        "        \"micro_f1\": f1_score(out_label_ids, preds, average=\"micro\"),\n",
        "        \"prediction\": preds,\n",
        "        \"labels\": out_label_ids,\n",
        "        \"proba\": proba\n",
        "    }\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-akID8m-m0LG"
      },
      "outputs": [],
      "source": [
        "optimizer_step = 0\n",
        "global_step = 0\n",
        "train_step = 0\n",
        "tr_loss, logging_loss = 0.0, 0.0\n",
        "best_valid_auc = 0.75\n",
        "best_valid_f1 = 0.70\n",
        "global_steps_list = []\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "val_acc_list = []\n",
        "val_auc_list = []\n",
        "val_f1_list = []\n",
        "eval_every = len(train_dataloader) // 7\n",
        "running_loss = 0\n",
        "file_path=\"models/\"\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "for i in range(num_train_epochs):\n",
        "    print(\"Epoch\", i+1, f\"from {num_train_epochs}\")\n",
        "    whole_y_pred=np.array([])\n",
        "    whole_y_t=np.array([])\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        model.train()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        labels = batch[5]\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[0],\n",
        "            \"input_modal\": batch[2],\n",
        "            \"attention_mask\": batch[1],\n",
        "            \"modal_start_tokens\": batch[3],\n",
        "            \"modal_end_tokens\": batch[4],\n",
        "            \"return_dict\": False\n",
        "        }\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "        loss = criterion(logits, labels)        \n",
        "        \n",
        "        if gradient_accumulation_steps > 1:\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "        loss.backward()\n",
        "        \n",
        "        tr_loss += loss.item()\n",
        "        running_loss += loss.item()\n",
        "        global_step += 1\n",
        "        \n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule         \n",
        "            \n",
        "            optimizer_step += 1\n",
        "            optimizer.zero_grad()   \n",
        "                        \n",
        "        if (step + 1) % eval_every == 0:\n",
        "            \n",
        "            average_train_loss = running_loss / eval_every\n",
        "            train_loss_list.append(average_train_loss)\n",
        "            global_steps_list.append(global_step)\n",
        "            running_loss = 0.0  \n",
        "            \n",
        "            val_result = evaluate(model, tokenizer, criterion, eval_dataloader)\n",
        "            \n",
        "            val_loss_list.append(val_result['loss'])\n",
        "            val_acc_list.append(val_result['accuracy'])\n",
        "            val_auc_list.append(val_result['AUC'])\n",
        "            val_f1_list.append(val_result['micro_f1'])\n",
        "            \n",
        "            # checkpoint\n",
        "            if val_result['micro_f1'] > best_valid_f1:\n",
        "                best_valid_f1 = val_result['micro_f1']\n",
        "                val_loss = val_result['loss']\n",
        "                val_acc = val_result['accuracy']\n",
        "                val_f1 = val_result['micro_f1']\n",
        "                model_path = f'{file_path}/model-embs{num_image_embeds}-seq{max_seq_length}-F1{best_valid_f1:.3f}-loss{val_loss:.3f}-acc{val_acc:.3f}.pt'\n",
        "                print(f\"F1 improved, so saving this model\")  \n",
        "                save_checkpoint(model_path, model, val_result['loss'])             \n",
        "            \n",
        "            print(\"Train loss:\", f\"{average_train_loss:.4f}\", \n",
        "                  \"Val loss:\", f\"{val_result['loss']:.4f}\",\n",
        "                  \"Val acc:\", f\"{val_result['accuracy']:.4f}\",\n",
        "                  \"F1 score:\", f\"{val_result['micro_f1']:.4f}\")   \n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTgn7ipH6Vzz"
      },
      "outputs": [],
      "source": [
        "# path for test files\n",
        "!unzip gdrive/MyDrive/test.zip -d \"/content/testing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEKqLUCc6BSw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "num_labels = 1\n",
        "data_dir = 'testing/test'\n",
        "test_batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clg_mXYCnf72"
      },
      "outputs": [],
      "source": [
        "class TestJsonlDataset(Dataset):\n",
        "        def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n",
        "            self.data = [json.loads(l) for l in open(data_path)]\n",
        "            self.data_dir = os.path.dirname(data_path)\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_seq_length = max_seq_length\n",
        "            self.transforms = transforms\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n",
        "            start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
        "            sentence = sentence[:self.max_seq_length]\n",
        "\n",
        "            id = torch.LongTensor([self.data[index][\"id\"]])        \n",
        "            image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
        "            sliced_images = slice_image(image, 288)\n",
        "            sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n",
        "            image = resize_pad_image(image, image_encoder_size)\n",
        "            image = np.array(self.transforms(image))        \n",
        "            sliced_images = [image] + sliced_images        \n",
        "            sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n",
        "\n",
        "            return {\n",
        "                \"image_start_token\": start_token,            \n",
        "                \"image_end_token\": end_token,\n",
        "                \"sentence\": sentence,\n",
        "                \"image\": sliced_images,\n",
        "                \"id\": id,\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgPscOzCnqMD"
      },
      "outputs": [],
      "source": [
        "def final_collate_fn(batch):\n",
        "        lens = [len(row[\"sentence\"]) for row in batch]\n",
        "        bsz, max_seq_len = len(batch), max(lens)\n",
        "\n",
        "        mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "        text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
        "\n",
        "        for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
        "            text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
        "            mask_tensor[i_batch, :length] = 1\n",
        "\n",
        "        img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
        "        id_tensor = torch.stack([row[\"id\"] for row in batch])\n",
        "        img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
        "        img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
        "\n",
        "        return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, id_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1aNjrNmnty3"
      },
      "outputs": [],
      "source": [
        "def load_test_examples(test_file=\"test_seen (1).jsonl\"):\n",
        "        path = os.path.join(data_dir, test_file)\n",
        "        dataset = TestJsonlDataset(path, tokenizer, preprocess, max_seq_length - num_image_embeds - 2)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2MR0oSFn-YE"
      },
      "outputs": [],
      "source": [
        "def final_prediction(model, dataloader): \n",
        "        preds = None\n",
        "        proba = None\n",
        "        all_ids = None\n",
        "        for batch in tqdm(dataloader):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                ids = batch[5]\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"input_modal\": batch[2],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"modal_start_tokens\": batch[3],\n",
        "                    \"modal_end_tokens\": batch[4],\n",
        "                    \"return_dict\": False\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs[0]\n",
        "            if preds is None:\n",
        "                all_ids = ids.detach().cpu().numpy()\n",
        "                preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()            \n",
        "            else:  \n",
        "                all_ids = np.append(all_ids, ids.detach().cpu().numpy(), axis=0)\n",
        "                preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > 0.5, axis=0)\n",
        "                proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        result = {\n",
        "            \"ids\": all_ids,\n",
        "            \"preds\": preds,\n",
        "            \"probs\": proba,\n",
        "        }\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEJgohowLwhD"
      },
      "outputs": [],
      "source": [
        "!mv \"/content/test_seen (2).jsonl\" \"/content/testing/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66QlNcseoFlX"
      },
      "outputs": [],
      "source": [
        "final_test = load_test_examples()\n",
        "final_test_sampler = SequentialSampler(final_test)\n",
        "final_test_dataloader = DataLoader(\n",
        "            final_test, \n",
        "            sampler=final_test_sampler, \n",
        "            batch_size=test_batch_size, \n",
        "            collate_fn=final_collate_fn\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8acf4c8fbd0f49b1823d03d1b03ff364",
            "5a8403781d004afe83973d047ff3d4ae",
            "cd4b2e3970824051ad54dd84f7143707",
            "4fd5d59bd13f4504afe594e54c2a030c",
            "612ef1a6a9c74b77ac129c700fc61d95",
            "7f174be246394a7591d0ac18d56ed2fe",
            "aa730a895b074574a70f4e359e4578cd",
            "187cd2e8fe3d425b9340efc2513447e4",
            "a87712eba0904f29b8adda4cdf9fbb36",
            "edb70bc44d1d4cebb4be6c15d370ffae",
            "42bff1f9058945b097d3cf6c43b04910"
          ]
        },
        "id": "yWYgCoVVoUM_",
        "outputId": "4780684f-9a9b-49d1-b599-92c85c18ff7d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8acf4c8fbd0f49b1823d03d1b03ff364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/63 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "results = final_prediction(model, final_test_dataloader)\n",
        "\n",
        "results['ids'] = results['ids'].reshape(-1)\n",
        "results['preds'] = results['preds'].reshape(-1)\n",
        "results['probs'] = results['probs'].reshape(-1)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df = df[['ids', 'probs', 'preds']]\n",
        "df.columns = ['id', 'proba', 'label']\n",
        "df.label = df.label.astype(int)\n",
        "\n",
        "df.to_excel('clip_stereo_pred.xlsx', index=False, float_format='%.3f')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZzquqNwKfjx"
      },
      "outputs": [],
      "source": [
        "df.to_excel('clip_stereo_pred.xlsx', index=False, float_format='%.3f')"
      ]
    }
  ]
}